{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90423cc7-ec9d-4513-861c-41f65ef3dd22",
   "metadata": {},
   "source": [
    "# Fine tuning DistilBert model with QLoRA and LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396391f0-eca7-4408-aec0-a78f5e14980b",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e41919-fd72-46d4-ba41-6c380bb6d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType, replace_lora_weights_loftq, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439e046a-2024-438c-8607-869e9f485428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44707493-1e91-4d7b-924f-ce29040e2eb4",
   "metadata": {},
   "source": [
    "## Define help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d360dd0-4502-4cc2-b482-b8e7c253d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help functions\n",
    "def save_to_json(data, file_path):\n",
    "    \"\"\"\n",
    "    Save a dictionary to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The dictionary to save.\n",
    "        file_path (str): The path to the JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    print(f\"Data successfully saved to {file_path}\")\n",
    "    \n",
    "    \n",
    "def load_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The data loaded from the JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68e3c2-d027-48db-a523-61b774d4f41b",
   "metadata": {},
   "source": [
    "## Load IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077cd67f-166d-4b64-acc3-9315be2d23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset\n",
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede8b43-bdea-453c-8990-e6aa02aad13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = imdb['train']['label']\n",
    "unique_labels = set(train_labels)\n",
    "print(\"\\nUnique labels in the dataset (class information):\")\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c286a1c3-433f-4383-ad46-5ded85632f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {0: \"negative\", 1: \"positive\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0671d2f1-847f-431c-b488-1611704b5196",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = imdb[\"train\"].shuffle(seed=42)\n",
    "test_dataset = imdb[\"test\"].shuffle(seed=42)\n",
    "print(f\"No. of training sample: {len(train_dataset)}\")\n",
    "print(f\"No. of testing sample: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5af25b-fe09-47d7-af06-5d3a6cec4a29",
   "metadata": {},
   "source": [
    "## Define Tokenizer and preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94049ffc-bcab-4d6c-996b-9f2baaf42e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736f2a2f-a770-4fd6-86b7-5dc1cd7bc158",
   "metadata": {},
   "source": [
    "## Define metrics calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efafe7dc-3b76-4944-83e2-1a5605880622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\", trust_remote_code=True)\n",
    "\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "\n",
    "   return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb497b0-b5ac-4700-b17a-0c57e250364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = dict((v,k) for k,v in id2label.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288414b-9e19-497a-927c-025ccdd8bb53",
   "metadata": {},
   "source": [
    "## Load DistilBert-base-uncased model and configure the model for QLoRA fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1bad4-d71e-4938-b0a4-3a109e1e2c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA model quantization and configuration\n",
    "# Configure BitsAndBytes\n",
    "config_bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # quantize the model to 4-bits when you load it\n",
    "    bnb_4bit_quant_type=\"nf4\", # use a special 4-bit data type for weights initialized from a normal distribution\n",
    "    bnb_4bit_use_double_quant=True, # nested quantization scheme to quantize the already quantized weights\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # use bfloat16 for faster computation\n",
    "    llm_int8_skip_modules=[\"classifier\", \"pre_classifier\"] #  Don't convert the \"classifier\" and \"pre_classifier\" layers to 8-bit\n",
    ")\n",
    "# Load a quantized version of a pretrained model\n",
    "\n",
    "model_qlora = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\",\n",
    "                                                                 id2label=id2label,\n",
    "                                                                 label2id=label2id,\n",
    "                                                                 num_labels=2,\n",
    "                                                                 quantization_config=config_bnb,\n",
    "                                                                 #low_cpu_mem_usage=True,\n",
    "                                                                 #device_map='cuda:0'\n",
    "                                                                )\n",
    "model_qlora = prepare_model_for_kbit_training(model_qlora)\n",
    "qlora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Specify the task type as sequence classification\n",
    "    r=8,  # Rank of the low-rank matrices\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    lora_dropout=0.1,  # Dropout rate  \n",
    "    target_modules=['q_lin','k_lin','v_lin'] # which modules\n",
    ")\n",
    "\n",
    "peft_model_qlora = get_peft_model(model_qlora, qlora_config)\n",
    "replace_lora_weights_loftq(peft_model_qlora)\n",
    "peft_model_qlora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ffc3d0-43ea-4da9-b32a-7249b4fe823d",
   "metadata": {},
   "source": [
    "## Load DistilBert-base-uncased model and configure the model for LoRA fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a30099-8c1e-43ca-a6c0-3c93e38fd7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA model configuration\n",
    "model_lora = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\",\n",
    "                                                                 id2label=id2label,\n",
    "                                                                 label2id=label2id,\n",
    "                                                                 num_labels=2,\n",
    "                                                                 #quantization_config=config_bnb,\n",
    "                                                                 #low_cpu_mem_usage=True\n",
    "                                                                )\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Specify the task type as sequence classification\n",
    "    r=8,  # Rank of the low-rank matrices\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    lora_dropout=0.1,  # Dropout rate  \n",
    "    target_modules=['q_lin','k_lin','v_lin'] # which modules\n",
    ")\n",
    "\n",
    "peft_model_lora = get_peft_model(model_lora, lora_config)\n",
    "peft_model_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799cbed9-f858-4ba2-96aa-f5d056c73198",
   "metadata": {},
   "source": [
    "## Configure QLoRA training arguments and train Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef9144-007d-4952-9983-310da494c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and train QLora\n",
    "training_args_qlora = TrainingArguments(\n",
    "    output_dir=\"./results_qlora\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    weight_decay=0.01,\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "trainer_qlora = Trainer(\n",
    "    model=peft_model_qlora,\n",
    "    args=training_args_qlora,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics)\n",
    "\n",
    "trainer_qlora.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97d1b3-803e-4e0d-99a4-9220a5340681",
   "metadata": {},
   "source": [
    "## Configure LoRA training arguments and train Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746bd162-2cbf-4c93-b479-8bdd446f46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and train Lora\n",
    "training_args_lora = TrainingArguments(\n",
    "    output_dir=\"./results_lora\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    weight_decay=0.01\n",
    ")\n",
    "trainer_lora = Trainer(\n",
    "    model=peft_model_lora,\n",
    "    args=training_args_lora,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics)\n",
    "\n",
    "trainer_lora.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ecb89-659c-418b-9dd1-0292dedcb61e",
   "metadata": {},
   "source": [
    "## Saving trained models and compare metrics of fine-tuned models with QLoRA and LoRA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be10630d-4f02-4ece-b2ec-8e911d322c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save QLoRA weight and evaluate\n",
    "trainer_qlora.save_model(\"./qlora_final_model\")\n",
    "log_history_qlora = trainer_qlora.state.log_history\n",
    "get_metric_qlora = lambda metric, log_history_qlora: [log[metric] for log in log_history_qlora if metric in log]\n",
    "eval_accuracy_qlora=get_metric_qlora('eval_accuracy',log_history_qlora)\n",
    "eval_loss_qlora=get_metric_qlora('eval_loss',log_history_qlora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3052b226-2099-41d9-9253-2aa7395e855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA weight and evaluate\n",
    "trainer_lora.save_model(\"./lora_final_model\")\n",
    "log_history_lora = trainer_lora.state.log_history\n",
    "get_metric_lora = lambda metric, log_history_lora: [log[metric] for log in log_history_lora if metric in log]\n",
    "eval_accuracy_lora=get_metric_lora('eval_accuracy',log_history_lora)\n",
    "eval_loss_lora=get_metric_lora('eval_loss',log_history_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59bef7-36cc-4ab3-9e42-8a4509dfb8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare QLoRA with LoRA\n",
    "plt.plot(eval_accuracy_qlora,label='accuracy_qlora', marker=\"o\", color='b')\n",
    "plt.plot(eval_loss_qlora,label='loss_qlora', marker=\"^\", color='b')\n",
    "plt.plot(eval_accuracy_lora,label='accuracy_lora', marker=\"o\", color='g')\n",
    "plt.plot(eval_loss_lora,label='loss_lora', marker=\"^\", color='g')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy and Loss with QLoRA and LoRA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
